\chapter{Conclusion \& Future Work}
In this study, we explored the potential of using Neural Radiance Fields (NeRFs) for synthesizing novel views of various scenes. Through experiments and analysis of the results, we aimed to provide insights into the capabilities and limitations of NeRFs for this task. Our findings indicate that the different NeRF methods (NeRF, mip-NeRF, instant-ngp, Nerfacto) perform differently on unbounded, bounded, real, and Blender scenes in terms of reconstruction quality and computational efficiency. We also found that increasing the number of input images can improve the performance of NeRF methods, as measured by metrics such as PSNR and SSIM, but this comes at the cost of longer processing times. In addition, our results show that Polycam outperforms COLMAP in terms of quality and efficiency when processing the same data, but viewing NeRFs in virtual reality remains a challenge. Overall, our findings suggest that NeRFs have the potential to be effective for synthesizing 3D scenes from 2D imagery, but there are still challenges and limitations that need to be addressed.

\section{Future work}
This section presents ideas related to NeRFs that were not covered in this report, but would be interesting to explore in future work.

\subsection{Large scale NeRF}
Scaling NeRFs to larger scenes isn't trivial. The underlying MLP only has a certain capacity, and if we were to increase the capacity, training times would increase and rendering times would scale linearly. Rendering is already an expensive operation which further supports the claim for another solution. We've discussed Block-NeRF which decouples rendering times and the ability to reconstruct large scenes by leveraging multiple NeRFs to reconstruct an area.

Compared to the other fields covered in NeRF research, there aren't a lot of papers exploring large-scale NeRFs. This might be due to the amount of data needed and the corresponding data capture endeavor. In future work, it would be interesting to explore the balance between representing scenes with a single NeRF and when it would make sense to split the scenes into separate NeRFs.

An interesting experiment would be to explore the impact of area size. Area in itself is poorly defined in the context of NeRF, since scale is perspective relative, depending on the level of detail you want. A single NeRF can efficiently represent a 3D model of the entire world, but the level of detail won't be satisfactory as you try to render detailed images. In such an experiment we could use a self-captured streetview-scene, like the one portrayed in \autoref{fig:streetview-dataset}, as the benchmark for area-related experiments.

\subsubsection{Large scale data collection}
In order to collect data for the large scale NeRF we have access to NAPLab's car. The car features eight calibrated cameras, resulting in a $360^{\circ}$ field of vision around the car. In combination with GNSS and GPS, the car's rig should enable capturing images with corresponding camera poses, within 1-3cm accuracy.

A car equipped with calibrated cameras and GNSS/GPS systems, such as the one described in the question, can be used to create a system of local coordinate systems in order to represent accurate camera poses for all the input images. First, the car's cameras can be used to capture images of the surrounding environment as the car moves. The positions and orientations of the cameras at the time the images are taken can be determined using the GNSS/GPS systems, which should enable the camera poses to be determined with an accuracy of 1-3 cm.

Next, these camera poses can be used to create a set of local coordinate systems, with each coordinate system being centered at a different camera position. Finally, these local coordinate systems can be transformed into a global coordinate system using coordinate transformation techniques. By leveraging transformation matrices that encode the relationship between the local and global coordinate systems, the camera poses in the local coordinate systems can be transformed into the global coordinate system. Further we would use techniques explored in Block-NeRF \cite{tancik_block-nerf_2022} in order to train and inference NeRFs representing large-scale scenes.

%We explore the impact of area size. Area in itself is poorly defined in the context of NeRF, since scale is perspective relative, depending on the level of detail you want. A single NeRF can efficiently represent a 3D model of the entire world, but the level of detail won't be satisfactory as you try to render detailed images. In this experiment we'll use a self-captured scene as the benchmark for area-related experiments. The scene \cite{data:streetview} is captured on a relatively straight street, bounded by houses on each side. 