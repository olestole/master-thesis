\chapter{Discussion}
This chapter discusses and analyzes the results of our experiments, highlighting their significance in the context of the research questions. We also reflect on any limitations of the thesis and attempt to address the research questions based on the findings.

\section{Defining a baseline}

\begin{comment}
Points to discuss:
- Main point: Discuss the process of choosing which experiments was chosen.
- There might have been other experiments that should've been included in deciding the baseline.
- The way to choose which experiment goes forward could've been done differently.
- Although one experiment seem to do better with the current chosen setup, it might've done worse in another setup.
\end{comment}

In order to define a baseline for capturing data for training NeRFs, I chose five experiments to test: camera setup, capacity, number of frames, image size, and vehicle speed. These experiments were chosen based on heuristics and knowledge of what contributes to good NeRF results, such as well-lit scenes and non-blurry images. While the chosen experiments consider important factors for capturing data for NeRFs, there may have been other experiments that could have been included in defining the baseline. It's also important to weigh the fact that the best-performing settings in one experiment may not generalize to other setups. Overall, the process of defining a baseline is an iterative one that requires careful consideration of various factors and a willingness to continuously improve and refine the baseline.

\subsection{Camera setup}

The camera setup used in the experiment was arbitrary and may not reflect how cameras are typically rigged on cars. Future research could explore more realistic camera setups to improve the generalizability of the results. Additionally, the camera setups used in the study were sparse, with only a few cameras at specific angles. It is possible that other camera setups could yield even better results.

The results of the experiment showed relatively little difference in the quantitative metrics across the camera setups tested. However, camera setup 1 with two cameras at -10$^{\circ}$ and 10$^{\circ}$ yaw produced the highest SSIM and lowest LPIPS scores, indicating that it produced the most visually similar and perceptually pleasing images. One possible explanation of this result can be the level of overlap between the captured images the given setup provides. Both cameras have a field of view of $90^\circ$ and are mounted in the same location, resulting in a $70^\circ$ overlap between the images in the training data. This overlap means that when the model trains on an image from one of the cameras, it necessarily also trains on approximately three-quarters of the scene captured from the other camera. Because the evaluation set is a subset of the training images, it's fair to assume that the model should score high on the respective metrics.


\begin{comment}
- The position of the camera was arbitrary. Could've done more research into how cameras on cars usually are rigged.
- The camera setups are very sparse, a lot of different possibilities.

Results:
- Relatively little difference in the quantitative results.
- Why did the -10 and 10 yaw yield the best SSIM and LPIPS?
- The evaluation images are a subset of the training images. Because the -10 and 10 have a lot of overlap, they have a lot of common training data which will allow the model to learn the scene which it is evaluated on, in turn yielding high scores on the chosen metrics.


This overlap allows the model to train and learn the scene which it is evaluated on, because the evaluation set is a subset of the training images, more than the other setups, and it'll naturally score high on the respective images.

the model to train on the partial scene with two times the amount of data, and since the evaluation set is a subset of the training images, it'll naturally score high on the respective images.

\end{comment}










\subsection{Capacity}
% Summarize the important discussion added in the experiment
As stated in the experiment's section, the longest segment was selected for the baseline despite achieving the lowest scores across the metrics. This was done to ensure that the scene encompasses a diverse range of environments, including straight roads, curves, intersections, and varying lighting conditions.

% New discussion
In the qualitative analysis of the capacity results, it's evident that the quality of the renders degrades as the segment's length increase. The most prominent deterioration is the increase in blur. A reason why PSNR remains relatively constant across the different experiments is probably in part because PSNR has been shown not to capture blur well \cite{videoprocessingai}, as exemplified by \autoref{fig:psnr-critique}. This example demonstrates the importance of evaluating the different metrics, as both SSIM and LPIPS are good metrics for capturing blur.

\input{figures/psnr-critique}

















\subsection{Number of frames}

%- The captured images are very similar
%- The default dataset size in Nerfstudio is 300 images
%- Show a mathematical proof of why more images would make sense

To comprehend the outcomes of this experiment, it would be beneficial to look into how the number of frames and the image size impact the training process. We train for 15'000 iterations where each iteration uses 4096 pixels. This results in $\sim61$ million pixels being viewed throughout a single training. With 225 images and an image resolution of $600 \times 450$, we have $\sim61$ million pixels. That means that by the end of the training, approximately all of the input pixels were trained on. A dataset containing more than 225 images and corresponding camera poses would leave abundant pixels, and any fewer would lead to pixels being trained on multiple times. This calculation would entail that the results should be relatively similar for all experiments with 1231, 615, 411, 307, and 247 images respectively. But, there is a significant drop in PSNR from experiments 1 to 4. The drop in PSNR leads me to believe another important factor is the variety in the dataset.


% TODO: I should discuss other options as to why the number of frames affect the metrics in the way it does.












\subsection{Image size}
Based on the metric scores, it might seem counterintuitive that the lowest resolution of $200 \times 150$ performed best. However, a deeper look at the qualitative results in \autoref{fig:image-size-comparison} reveals a different narrative. Despite lower resolutions yielding higher scores on metrics, they seem to fail in capturing fine-grained details, resulting in less visually pleasing images. On the other hand, higher-resolution images allow the NeRF model to capture and replicate more intricate details, leading to superior visual outcomes, albeit with lower metric scores. This difference can be attributed to the higher sensitivity of metrics to minor discrepancies and noise in high-resolution images, potentially causing significant metric score reductions despite only minimal perceptual differences.

Given these considerations, the chosen resolution of $400 \times 300$ for the following experiments is a balanced choice, considering both the metrics and the perceptual image quality. Furthermore, it aligns well with the chosen number of frames from the previous experiment, as the combined configuration will allow training to cover about 84\% of the input pixels.

\begin{comment}
The qualitative assessment shows clear evidence of higher resolution images leading to higher fidelity renders, although the metrics suggest otherwise. When using lower-resolution images, the metrics may become less sensitive because they are less affected by small differences between the synthesized and ground-truth images. This is because lower-resolution images have fewer pixels, which can make the metrics less precise in measuring the perceptual similarity between the images. However, using lower-resolution images can also lead to a loss of detail and fidelity in the synthesized images. When comparing low-resolution images with high-resolution images, these metrics may become less effective because the high-resolution images are more sensitive to small differences between the synthesized and ground-truth images. In other words, the NeRF may generate high-quality images that are perceptually similar to the ground-truth images, but small differences in the pixel values or noise can cause a significant decrease in the metric scores.

The chosen resolution of $400 \times 300$ aligns well with the chosen number of frames from the previous experiment. With 2 ticks per image, $\sim615$ images for the baseline, we have $\sim73$ million pixels. The training will cover about 84\% of the input pixels.

This might be a special case for synthetic data where we have perfect camera poses. With higher-resolution images, the requirement for accurate camera poses increases as the camera poses have to be aligned pixel perfect with the image.
\end{comment}












\subsection{Vehicle speed}
The reason why the faster runs achieve worse results might be attributed to the motion blur and temporal artifacts that can occur when the vehicle is moving fast. In CARLA these effects are added as part of the post-processing of the captured camera image. At higher speeds, the motion of the vehicle can cause blurring and distortion in the captured images, which can reduce the quality of the data and make it more difficult for the NeRF to learn the underlying 3D scene structure and appearance. In contrast, slower vehicle speeds can reduce the amount of motion blur and temporal artifacts, resulting in clearer and more detailed images. Another side effect of driving slower is that it leads to an increased amount of images captured. At 50\% speed, the dataset consists of 1095 images, in contrast to the 429 images captured at 200\% speed.


% The reason for this can be attributed to the motion blur and temporal artifacts that can occur when the vehicle is moving too fast. At higher speeds, the motion of the vehicle can cause blurring and distortion in the captured images, which can reduce the quality of the data and make it more difficult for the NeRF to learn the underlying 3D scene structure and appearance. In contrast, slower vehicle speeds can reduce the amount of motion blur and temporal artifacts, resulting in clearer and more detailed images. Another side effect of driving slower is that it leads to increased amount of images captured. At 50\% speed, the dataset consists of 351 images, in contrast to the 131 images captured at 200\% speed.
























\subsection{Combined baseline}
% I might not need the initial "Defining a baseline" if I sum it up in this chapter.
\begin{description}[leftmargin=!,labelwidth=\widthof{RQ 1:}]
\item[\textbf{RQ 1:}] What are the critical factors that need to be considered when capturing synthetic data for training NeRF models, and how do they impact the performance of the resulting models?
\end{description}


From the experiments discussed above, it's evident that they all contribute to the quality of the data capture, which in turn contributed to the quality of the image synthesis from the resulting NeRF. Nevertheless, it is challenging to quantify the extent to which each of the different configurations affects the quality of the final baseline results.

Looking at the experiments separately, image resolution had the largest span between the quantitatively best and worst metrics. Nevertheless, the qualitative results indicated that the quantitative comparison didn't convey a fair comparison of the render-quality. Due to this, the capacity-experiment could be the experiment with the most impact on the final result.

In conclusion, when capturing synthetic data for training NeRF models, it's important to consider the camera setup, segment length or scene size, dataset size, image size, and vehicle speed. Each of these parameters presents unique influences on the quality of the data captured, and in turn, the performance of the resulting NeRF models.


\begin{comment}
The critical factors that must be considered when capturing synthetic data for training NeRF models, as inferred from the discussed baseline experiments, encompass camera setup, capacity, number of frames, image size, and vehicle speed. Each of these factors demonstrates a unique impact on the performance of the resulting models.

Camera setup plays a significant role in determining the quality of data captured for NeRF models. The experiments revealed that certain camera arrangements, specifically a setup with two cameras at -10$^{\circ}$ and 10$^{\circ}$ yaw, can result in superior SSIM and LPIPS scores, indicating more visually similar and perceptually pleasing images. This finding can be attributed to the degree of overlap between images captured by each camera in the setup. However, the experiments also highlight that camera arrangements were arbitrary and future research may investigate more realistic setups to better generalize the results.

The capacity, or the length of the road segment covered, also impacts the performance of the NeRF models. Even though longer segments led to lower scores across the metrics, these were still selected as the baseline to ensure a diverse range of environmental exposure for the model. It was noticed that the quality of renders deteriorated, especially in terms of increased blur, as segment length increased. Hence, the capacity of data has implications on the clarity and diversity of the training set, which could potentially influence the robustness of the model to diverse scenarios.

The number of frames in the dataset is another crucial factor. An optimal balance between the number of images and the image resolution is important for efficient training. In the experiments, a dataset with 225 images, each with a resolution of $600 \times 450$, covered approximately all of the input pixels by the end of training. However, a dataset with too many or too few images could result in either underutilization or overutilization of the pixels during training. Additionally, a decrease in PSNR with a drop in the number of frames points to the role of variety in the dataset.

The size of the images, or their resolution, has a complex impact on model performance. While higher resolution images contribute to higher fidelity renders, the metrics could suggest otherwise due to their increased sensitivity to minor differences in synthesized and ground-truth images. Hence, resolution can impact the quality of synthesized images and their metric scores, and it is crucial to align it with the number of frames for efficient coverage of input pixels.

Vehicle speed is a significant factor when capturing synthetic data, particularly due to the influence of motion blur and temporal artifacts on image quality. Faster vehicle speeds can lead to increased blur and distortion, making it challenging for the NeRF to learn the scene structure and appearance. Conversely, slower speeds provide clearer and more detailed images, improving the quality of the training data. Furthermore, slower speeds result in a larger dataset due to the increased number of images captured.

In conclusion, each of these factors—camera setup, capacity, number of frames, image size, and vehicle speed—has a unique and important influence on the quality of synthetic data captured for training NeRF models. Their careful consideration is essential for optimizing the performance of the resulting models.
\end{comment}








\section{Adding noise}
\begin{description}[leftmargin=!,labelwidth=\widthof{RQ 1:}]
\item[\textbf{RQ 2:}] How does the accuracy of initial camera poses and segment size influence the effectiveness of camera pose optimization, and is it possible to achieve equivalent results by optimizing rough camera poses directly, as opposed to pre-processing an approximation of accurate camera poses with tools such as COLMAP?
\end{description}

This research question addresses the effects of imperfect camera poses, frequently encountered in real-world scenarios due to GPS/GNSS inaccuracies, on the performance of camera pose optimization. We examine this by adding Gaussian noise to camera poses obtained from the CARLA pipeline, simulating real-world errors in the accuracy of these poses.

Despite Gaussian noise not perfectly replicating real-world noise distributions, its application allows a meaningful examination of camera pose optimization within our Nerfacto-pipeline. It is noteworthy that while the quantitative distinction between the non-optimized and optimized camera pose results might appear marginal, the qualitative contrasts outlined in both \autoref{fig:noise-short-segments} and \autoref{fig:noise-baseline-segments} deliver strong evidence of the camera pose optimization's effectiveness.

Particularly, the qualitative comparison highlights that the optimized camera poses generate consistently higher quality renders, even under significant noise levels. This is more discernible within shorter segments, where the optimization seems more efficient than in larger baseline scenes. This finding may be attributed to the previously discussed limitations in capacity, given that the camera optimization treats camera pose parameters as jointly optimized learnable parameters along with the RGB values.

Interestingly, the only instance where non-optimized poses yield superior results is when no noise is introduced. In such cases, the resulting render from non-optimized camera poses appears considerably sharper than its optimized counterpart, which may appear somewhat blurred. However, this outcome is most likely unique to synthetic datasets with perfect camera poses.


When examining the effectiveness of COLMAP pre-processing against the joint optimization of initial camera poses along with the other learnable parameters, it becomes evident that COLMAP tends to deliver superior performance. This observation holds particularly when the initial rough camera poses are influenced by a rather minor noise adhering to a Gaussian distribution with a standard deviation of $0.1^2$. Under these conditions, all three key metrics (PSNR, SSIM, LPIPS) associated with the NeRF lag behind those achieved by COLMAP, as can be seen from \autoref{tab:colmap-vs-poses} and \autoref{tab:exp-gaussian-noise}. Furthermore, when the noise's standard deviation is escalated to $0.5^2$, which is fair to assume could occur during real-world data capture, the metrics degrade even more significant compared to what COLMAP can deliver. While the processing time required by COLMAP might seem prohibitively lengthy for large datasets, this concern can be addressed by splitting the data into smaller sections. Thus, despite the initial time investment, COLMAP's superior performance merits consideration, particularly in scenarios with significant noise in initial camera poses.

\begin{comment}
The horizontal and vertical vectors in each experiment are subject to equal Gaussian noise. This approach may not fully capture the variability in noise that arises from realistic data capture scenarios, where factors such as the direction and speed of the vehicle can affect noise distribution. Nevertheless, the use of Gaussian noise enables the evaluation of the impact of camera optimization in the Nerfacto-pipeline.

While the difference in quantitative results between the runs with- and without camera pose optimization appear small, the qualitative comparison depicted in both \autoref{fig:noise-short-segments} and \autoref{fig:noise-baseline-segments} provides compelling evidence of its efficacy. Contrary to the NeRF that doesn't optimize the camera poses, the renders from the NeRF that optimized the camera poses remain relatively high-quality even when there is a substantial amount of noise added. This is particularly noticeable on shorter segments, where the optimization appears to perform even better than on the larger baseline scene. The observation can likely be ascribed to the capacity limitations previously discussed. This is because the camera optimization approach treats the camera pose parameters as learnable parameters that are jointly optimized with the RGB values.

The only scenario where NeRF with non-optimized camera poses outperforms its optimized counterpart is when no noise is added. In such cases, the resulting render is notably sharper than that produced by the optimized NeRF, which can appear more blurred. This finding is likely exclusive to synthetic datasets with perfect camera poses.
\end{comment}


\begin{comment}
DON'T NEED THIS SECTION BECAUSE THE COLMAP-RESULTS ARE DISCUSSED IN COMBINATION WITH THE NOISE-EXPERIMENT ABOVE.

\section{COLMAP vs. Absolute poses}
From the quantitative results, it appears COLMAP optimizes the camera poses similar to the camera pose optimization step in the Nerfacto-pipeline. When we turn off the camera pose optimization step, the perfect poses from CARLA enable even higher scores across the metrics and sharper renders.
\end{comment}





\section{Different models}
\begin{description}[leftmargin=!,labelwidth=\widthof{RQ 1:}]
\item[\textbf{RQ 3:}] How do different NeRF methods (NeRF, mip-NeRF, instant-ngp, Nerfacto) perform on unbounded scenes in terms of reconstruction quality and computational efficiency?
\end{description}


TODO



\section{Block NeRF}
\begin{description}[leftmargin=!,labelwidth=\widthof{RQ 1:}]
\item[\textbf{RQ 4:}] What are the technical challenges and considerations for implementing a functional approach for large-scale NeRF within the Nerfstudio API, and how does it compare to approaches not optimized for large-scale in terms of scalability, efficiency, and rendering quality?
\end{description}

Although the initial Block NeRF implementation is naive, both the quantitative and qualitative results provided demonstrate its efficacy. The renders of the shorter segments are much sharper and provide an overall better result. Although the results are very good, it's worth noting that the implementation requires substantially more computing. 





\subsection{Improving Block NeRF}
%The overlap between the blocks become substantially less visible with higher overlap values, $\delta$. When we increase $\delta$, we increase the dataset of each block leading to the capacity issue previously explored. In a production setting, it'll be important to tune all these parameters to effectively maximize quality across all blocks.

Although we solve the issue related to overlap, we see from \autoref{fig:block-nerf-frame-comparison} that the last frame in a block provides noticeably lower quality renders than the first frame in the next block. This is to be expected if the second block's data has been captured near the respective motive seen from both blocks. This difference could probably be further mitigated by experimenting with image-merging techniques, e.g. by rendering the view from both blocks and merging them with inverse distance weighing.

%Further improvements that could be done:
%- The details of the scenery in the second frame is clearly better than the first, which is expected as block number 2 has been trained on close-up images of the scene. This difference could be mitigated by looking into image-merging techniques such as inverse distance weighing.





\section{Implementation of CARLA to Nerfstudio pipeline}
Discuss the implementation.











\section{Generation of imperfect datasets}
\textbf{Unsure where to put this section}

One important application of NeRFs is their ability to synthesize images from previously unseen viewpoints, not present in the original dataset. This capability could prove particularly useful in applications where you need to expand a dataset, e.g. in a machine learning setting, much like you would employ data augmentation.

Given this thesis' focus on data captured by a driving vehicle, an apparent application is the generation of previously unseen vehicle trajectories. By capturing images from multiple viewpoints along a particular vehicle trajectory and subsequently training a NeRF on this data, the NeRF would enable the synthesis of new images depicting a previously unseen vehicle trajectory. This can be useful for a variety of applications, such as training autonomous driving systems. In \autoref{fig:nerfstudio-trajectory} you can see an example of how data from such an unseen trajectory can be generated by creating a camera path.

\input{figures/nerfstudio-trajectory}














\section{Shortcomings}
\textbf{Just notes for now}
\subsection{Test the transferability from synthetic to real data}
There wasn't enough time to thoroughly test if the findings from the synthetic data would transfer to real data and contribute to better-performing NeRFs.